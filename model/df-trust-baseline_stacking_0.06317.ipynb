{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "\n",
    "#特征重要性\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    " \n",
    "#聚合求count\n",
    "def feature_count(data, features=[]):\n",
    "    if len(set(features)) != len(features):\n",
    "        print('equal feature !!!!')\n",
    "        return data\n",
    "    new_feature = 'count'\n",
    "    for i in features:\n",
    "        new_feature += '_' + i.replace('add_', '')\n",
    "    try:\n",
    "        del data[new_feature]\n",
    "    except:\n",
    "        pass\n",
    "    temp = data.groupby(features).size().reset_index().rename(columns={0: new_feature})\n",
    "    data = data.merge(temp, 'left', on=features)\n",
    "    new_num_features.append(new_feature)\n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train_dataset.csv')\n",
    "test = pd.read_csv('../input/test_dataset.csv')\n",
    "\n",
    "#删除id\n",
    "test_id = test['用户编码'].copy()\n",
    "\n",
    "train.drop(\"用户编码\", axis = 1, inplace = True)\n",
    "test.drop(\"用户编码\", axis = 1, inplace = True)\n",
    "\n",
    "label = train['信用分'].copy()\n",
    "train.drop(['信用分'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#特征工程\n",
    "data = pd.concat([train, test])\n",
    "\n",
    "#原始的类别和数值特征\n",
    "ori_cat_features = ['用户实名制是否通过核实','是否大学生客户','是否黑名单客户','是否4G不健康客户','缴费用户当前是否欠费缴费','是否经常逛商场的人','当月是否逛过福州仓山万达',\n",
    "                    '当月是否到过福州山姆会员店','当月是否看电影','当月是否景点游览','当月是否体育场馆消费']\n",
    "ori_num_features = ['用户年龄','用户网龄（月）','用户最近一次缴费距今时长（月）','缴费用户最近一次缴费金额（元）','用户近6个月平均消费值（元）','用户账单当月总费用（元）',\n",
    "                    '用户当月账户余额（元）','用户话费敏感度','当月通话交往圈人数','近三个月月均商场出现次数','当月网购类应用使用次数','当月物流快递类应用使用次数',\n",
    "                    '当月金融理财类应用使用总次数','当月视频播放类应用使用次数','当月飞机类应用使用次数','当月火车类应用使用次数','当月旅游资讯类应用使用次数']\n",
    "ori_col = data.columns.tolist()\n",
    "                    \n",
    "#对年龄异常值取众数填充\n",
    "data.loc[data['用户年龄']==0, '用户年龄'] = data['用户年龄'].mode()\n",
    "\n",
    "#年龄特征\n",
    "data['网龄/年龄'] = data['用户网龄（月）'] / data['用户年龄']\n",
    "data['网龄年龄差'] = data['用户年龄'] - data['用户网龄（月）']/12\n",
    "\n",
    "#对金额相关特征做组合\n",
    "data['缴费金额是否能覆盖当月账单'] = data['缴费用户最近一次缴费金额（元）'] - data['用户账单当月总费用（元）']\n",
    "data['最近一次交费是否超过平均消费额'] = data['缴费用户最近一次缴费金额（元）'] - data['用户近6个月平均消费值（元）']\n",
    "data['当月账单是否超过平均消费额'] = data['用户账单当月总费用（元）'] - data['用户近6个月平均消费值（元）']\n",
    "data['缴费习惯'] = data['缴费用户最近一次缴费金额（元）'] / (data['用户近6个月平均消费值（元）'] + 0.001)\n",
    "data['通话人均花费'] = data['用户账单当月总费用（元）'] / (data['当月通话交往圈人数']+1)\n",
    "data['近半年账单'] = data['用户近6个月平均消费值（元）']*6 + data['用户账单当月总费用（元）']\n",
    "data['最近账单稳定性'] = data['用户账单当月总费用（元）'] / (data['用户近6个月平均消费值（元）'] + 0.001)\n",
    "data['费用/余额'] = data['用户账单当月总费用（元）'] / (data['缴费用户最近一次缴费金额（元）'] + 0.001)\n",
    "data['账户余额利用率'] = data['用户账单当月总费用（元）'] / (data['用户当月账户余额（元）'] + 0.001)\n",
    "\n",
    "#对次数特征做组合\n",
    "data['交通类应用使用次数'] = data['当月飞机类应用使用次数'] + data['当月火车类应用使用次数']\n",
    "\n",
    "new_num_features = [i for i in data.columns.tolist() if i not in ori_col]\n",
    "\n",
    "#充值金额是整数，和小数，应该对应不同的充值途径\n",
    "def top_up_amount_method(s):\n",
    "    \n",
    "    if(s == 0):\n",
    "        return 0\n",
    "    elif(s % 10 == 0):\n",
    "        return 1\n",
    "    elif((s / 0.998) % 10 ==0):\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "data['充值方式1'] = data['缴费用户最近一次缴费金额（元）'].apply(top_up_amount_method)\n",
    "\n",
    "def real_top_up_amount(s):\n",
    "    if((s / 0.998) % 10 ==0):\n",
    "        return s/0.998\n",
    "    else:\n",
    "        return s\n",
    "data[\"充值方式2\"] = data['缴费用户最近一次缴费金额（元）'].apply(real_top_up_amount)\n",
    "\n",
    "#对类别特征进行组合，是否可以得出更好的结果\n",
    "data['是否去过高档商场'] = data['当月是否逛过福州仓山万达'] + data['当月是否到过福州山姆会员店']\n",
    "data['是否去过高档商场'] = data['是否去过高档商场'].map(lambda x:1 if x>=1 else 0)\n",
    "data['是否_商场_电影'] = data['是否去过高档商场'] * data['当月是否看电影']\n",
    "data['是否_商场_旅游'] = data['是否去过高档商场'] * data['当月是否景点游览']\n",
    "data['是否_商场_体育馆'] = data['是否去过高档商场'] * data['当月是否体育场馆消费']\n",
    "data['是否_电影_体育馆'] = data['当月是否看电影'] * data['当月是否体育场馆消费']\n",
    "data['是否_电影_旅游'] = data['当月是否看电影'] * data['当月是否景点游览']\n",
    "data['是否_旅游_体育馆'] = data['当月是否景点游览'] * data['当月是否体育场馆消费']\n",
    "data['是否_商场_旅游_体育馆'] = data['是否去过高档商场'] * data['当月是否景点游览'] * data['当月是否体育场馆消费']\n",
    "data['是否_商场_电影_体育馆'] = data['是否去过高档商场'] * data['当月是否看电影'] * data['当月是否体育场馆消费']\n",
    "data['是否_商场_电影_旅游'] = data['是否去过高档商场'] * data['当月是否看电影'] * data['当月是否景点游览']\n",
    "data['是否_体育馆_电影_旅游'] = data['当月是否体育场馆消费'] * data['当月是否看电影'] * data['当月是否景点游览']\n",
    "data['是否_商场_体育馆_电影_旅游'] = data['是否去过高档商场'] * data['当月是否体育场馆消费'] * data['当月是否看电影'] * data['当月是否景点游览']\n",
    "\n",
    "new_cat_features = [i for i in data.columns.tolist() if i not in ori_col and i not in new_num_features]\n",
    "\n",
    "#对一些特征分段\n",
    "discretize_features=['交通类应用使用次数','当月物流快递类应用使用次数','当月飞机类应用使用次数','当月火车类应用使用次数','当月旅游资讯类应用使用次数']\n",
    "def map_discretize(x):\n",
    "        if x==0:\n",
    "            return 0\n",
    "        elif x<=5:\n",
    "            return 1\n",
    "        elif x<=15:\n",
    "            return 2\n",
    "        elif x<=50:\n",
    "            return 3\n",
    "        elif x<=100:\n",
    "            return 4\n",
    "        else:\n",
    "            return 5\n",
    "        \n",
    "for col in discretize_features:\n",
    "    data[col]=data[col].map(lambda x:map_discretize(x))\n",
    "\n",
    "#离散化\n",
    "transform_value_feature=['用户年龄','用户网龄（月）','当月通话交往圈人数','近三个月月均商场出现次数','当月网购类应用使用次数','当月物流快递类应用使用次数'\n",
    "                            ,'当月金融理财类应用使用总次数','当月视频播放类应用使用次数','当月飞机类应用使用次数','当月火车类应用使用次数','当月旅游资讯类应用使用次数']\n",
    "user_fea=['缴费用户最近一次缴费金额（元）','用户近6个月平均消费值（元）','用户账单当月总费用（元）','用户当月账户余额（元）']\n",
    "log_features=['当月网购类应用使用次数','当月金融理财类应用使用总次数','当月物流快递类应用使用次数','当月视频播放类应用使用次数']\n",
    "\n",
    "for col in transform_value_feature+log_features:\n",
    "    #取出最高99.9%值\n",
    "    ulimit=np.percentile(data[col].values,99.9)\n",
    "    #取出最低0.1%值\n",
    "    llimit=np.percentile(data[col].values,0.1)\n",
    "    data.loc[data[col]>ulimit,col]=ulimit\n",
    "    data.loc[data[col]<llimit,col]=llimit\n",
    "    \n",
    "for col in user_fea+transform_value_feature+log_features:\n",
    "    data[col]=data[col].map(lambda x:np.log1p(x))\n",
    "    \n",
    "#聚合特征\n",
    "# data = feature_count(data, ['用户年龄'])\n",
    "# data = feature_count(data, ['用户网龄（月）'])\n",
    "# data = feature_count(data, ['用户最近一次缴费距今时长（月）'])\n",
    "# data = feature_count(data, ['缴费用户最近一次缴费金额（元）'])\n",
    "# data = feature_count(data, ['用户近6个月平均消费值（元）'])\n",
    "# data = feature_count(data, ['用户账单当月总费用（元）'])\n",
    "# data = feature_count(data, ['用户话费敏感度'])\n",
    "# data = feature_count(data, ['当月通话交往圈人数'])\n",
    "# data = feature_count(data, ['近三个月月均商场出现次数'])\n",
    "# data = feature_count(data, ['最近一次交费是否超过平均消费额'])\n",
    "# data = feature_count(data, ['当月账单是否超过平均消费额'])\n",
    "\n",
    "# data = feature_count(data, ['用户话费敏感度','用户年龄'])\n",
    "# data = feature_count(data, ['用户话费敏感度','用户网龄（月）'])\n",
    "# data = feature_count(data, ['用户话费敏感度','用户最近一次缴费距今时长（月）'])\n",
    "# data = feature_count(data, ['用户话费敏感度','缴费用户最近一次缴费金额（元）'])\n",
    "# data = feature_count(data, ['用户话费敏感度','用户近6个月平均消费值（元）'])\n",
    "# data = feature_count(data, ['用户话费敏感度','用户账单当月总费用（元）'])\n",
    "# data = feature_count(data, ['用户话费敏感度','当月通话交往圈人数'])\n",
    "# data = feature_count(data, ['用户话费敏感度','近三个月月均商场出现次数'])\n",
    "# data = feature_count(data, ['用户话费敏感度','最近一次交费是否超过平均消费额'])\n",
    "# data = feature_count(data, ['用户话费敏感度','当月账单是否超过平均消费额'])\n",
    "\n",
    "# #聚合其他列的特征\n",
    "# sparse_feature = ['用户年龄','用户网龄（月）','用户最近一次缴费距今时长（月）','用户话费敏感度']\n",
    "# dense_feature = ['缴费用户最近一次缴费金额（元）','用户近6个月平均消费值（元）','用户账单当月总费用（元）',\n",
    "#                     '用户当月账户余额（元）']\n",
    "\n",
    "# def get_new_columns(name,aggs):\n",
    "#     l=[]\n",
    "#     for k in aggs.keys():\n",
    "#         for agg in aggs[k]:\n",
    "#             if str(type(agg))==\"<class 'function'>\":\n",
    "#                 l.append(name + '_' + k + '_' + 'other')\n",
    "#             else:\n",
    "#                 l.append(name + '_' + k + '_' + agg)\n",
    "#     return l\n",
    "# for d in tqdm(sparse_feature):\n",
    "#     aggs={}\n",
    "#     for s in sparse_feature:\n",
    "#         aggs[s]=['count','nunique']\n",
    "#     for den in dense_feature:\n",
    "#         aggs[den]=['mean','max','min','std']\n",
    "#     aggs.pop(d)\n",
    "#     temp=data.groupby(d).agg(aggs).reset_index()\n",
    "#     temp.columns=[d]+get_new_columns(d,aggs)\n",
    "#     new_num_features.append(get_new_columns(d,aggs))\n",
    "#     data=pd.merge(data,temp,on=d,how='left')\n",
    "\n",
    "#记录特征\n",
    "cat_features = new_cat_features + ori_cat_features\n",
    "num_features = new_num_features + ori_num_features\n",
    "\n",
    "for i in cat_features:\n",
    "    data[i] = data[i].astype('category')\n",
    "for i in num_features:\n",
    "    data[i] = data[i].astype('float')\n",
    "\n",
    "# #类别特征做one-hot    \n",
    "# for feature in cat_features:\n",
    "#     try:\n",
    "#         data[feature] = LabelEncoder().fit_transform(data[feature].apply(int))\n",
    "#     except:\n",
    "#         data[feature] = LabelEncoder().fit_transform(data[feature])    \n",
    " \n",
    "\n",
    "train = data[:train.shape[0]]\n",
    "test = data[train.shape[0]:]    \n",
    "\n",
    "\n",
    "# train_x=train[num_features]\n",
    "# test_x=test[num_features]\n",
    "# enc = OneHotEncoder()\n",
    "# for feature in cat_features:\n",
    "#     enc.fit(data[feature].values.reshape(-1, 1))\n",
    "#     train_a= enc.transform(train[feature].values.reshape(-1, 1))\n",
    "#     test_a = enc.transform(test[feature].values.reshape(-1, 1))\n",
    "#     train= sparse.hstack((train_x, train_a), 'csr')\n",
    "#     test = sparse.hstack((test_x, test_a), 'csr')\n",
    "    \n",
    "    \n",
    "# #CountVectorizer()特征,在本题中没有合适的量\n",
    "# vector_feature = []\n",
    "# # for i in vector_feature:\n",
    "# #     data[i] = data[i].astype('str')\n",
    "# #     train[i] = train[i].astype('str')\n",
    "# #     test[i] = test[i].astype('str')\n",
    "\n",
    "    \n",
    "\n",
    "# cv=CountVectorizer()\n",
    "# for feature in vector_feature:\n",
    "#     cv.fit(data[feature])\n",
    "#     train_a = cv.transform(train[feature])\n",
    "#     test_a = cv.transform(test[feature])\n",
    "#     train = sparse.hstack((train_x, train_a), 'csr')\n",
    "#     test = sparse.hstack((test_x, test_a), 'csr')\n",
    "\n",
    "\n",
    "#开始训练\n",
    "# kf = StratifiedKFold(n_splits=5, random_state=2019, shuffle=False)\n",
    "# training_time = 0 \n",
    "# feature_importance_df = pd.DataFrame()\n",
    "# best_score = []\n",
    "# sub_list = []\n",
    "\n",
    "# clf = lgb.LGBMRegressor(\n",
    "#           boosting_type='gbdt', num_leaves=31, reg_alpha=2.2, reg_lambda=1.5,\n",
    "#           max_depth=-1, n_estimators=2000,\n",
    "#           subsample=0.8, colsample_bytree=0.7, subsample_freq=1,\n",
    "#           learning_rate=0.03, random_state=2019, n_jobs=-1)\n",
    "\n",
    "# for i, (train_index, val_index) in enumerate(kf.split(train, label)):\n",
    "#      t0 = time.time()\n",
    "#      X_train, y_train = train.loc[train_index,:], label[train_index]\n",
    "#      X_val, y_val     = train.loc[val_index,:],   label[val_index]\n",
    "#      #X_train, y_train = train[train_index], label[train_index]\n",
    "#      #X_val,    y_val   = train[val_index],   label[val_index]\n",
    "#      #clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)],\\\n",
    "#       #        eval_metric='mae', early_stopping_rounds=200, verbose=200, categorical_feature=cat_features)\n",
    "#      clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)],\\\n",
    "#               eval_metric='mae', early_stopping_rounds=200, verbose=200)\n",
    "#      pred_val = clf.predict(X_val, num_iteration=clf.best_iteration_)\n",
    "#      vali_mae = mean_absolute_error(y_val, np.round(pred_val))\n",
    "#      best_score.append(1/(1+vali_mae))\n",
    "#      pred_test = clf.predict(test,num_iteration=clf.best_iteration_)\n",
    "     \n",
    "#      fold_importance_df = pd.DataFrame()\n",
    "#      fold_importance_df[\"feature\"] = list(X_train.columns)\n",
    "#      fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "#      fold_importance_df[\"fold\"] = i + 1\n",
    "#      feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "     \n",
    "#      sub_list.append(pred_test)\n",
    "#      t = (time.time() - t0) / 60\n",
    "#      training_time += t\n",
    "     \n",
    "#      print(\"This round cost time:{:.2f} minutes, lgb scor:{:.8f},\\n\".format(t, 1/(1+vali_mae)))\n",
    "        \n",
    "# pred_test = np.mean(np.array(sub_list), axis=0)\n",
    "# print(best_score, '\\n', np.mean(best_score), np.std(best_score))\n",
    "# print(\"Total training time cost:{:.2f} minutes\".format(training_time))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "a962ce5d177c39973901bbffffd6c1482dc4b25a"
   },
   "outputs": [],
   "source": [
    "for i in cat_features:\n",
    "    train[i] = train[i].astype('int')\n",
    "    test[i]  = test[i].astype('int')\n",
    "    \n",
    "train = train.fillna(0)\n",
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "6d771c7c1a052cd09c5d51bc01184029601ffd60"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "d50842295b0f3a97517cc040d86a87938f0e7abf"
   },
   "outputs": [],
   "source": [
    "X = train.copy()\n",
    "y =label\n",
    "\n",
    "kfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def mae(y, y_pred):\n",
    "    return mean_absolute_error(y, y_pred)\n",
    "\n",
    "def cv_mae(model, X=X):\n",
    "    mae = -cross_val_score(model, X, y, scoring=\"neg_mean_absolute_error\", cv=kfolds)\n",
    "    return (mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "eb90f0cb6f2075079bad068bf6a4b4e3922cc63c"
   },
   "outputs": [],
   "source": [
    "alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\n",
    "alphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n",
    "e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n",
    "e_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "b6685e05bf7adecbf0f8cb0786a4f149b3b037c1"
   },
   "outputs": [],
   "source": [
    "ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\n",
    "lasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds))\n",
    "elasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))                                \n",
    "svr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "25cb914ad14630899813b75173918fcf90abd79e"
   },
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "a6cab18400e91fa1a06094eb0331ad7aaf21ce73"
   },
   "outputs": [],
   "source": [
    "lightgbm = LGBMRegressor(objective='regression', \n",
    "                                       num_leaves=4,\n",
    "                                       learning_rate=0.01, \n",
    "                                       n_estimators=5000,\n",
    "                                       max_bin=200, \n",
    "                                       bagging_fraction=0.75,\n",
    "                                       bagging_freq=5, \n",
    "                                       bagging_seed=7,\n",
    "                                       feature_fraction=0.2,\n",
    "                                       feature_fraction_seed=7,\n",
    "                                       verbose=-1,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "ff2d566472244dd8e3c3d7c70087d53443904460"
   },
   "outputs": [],
   "source": [
    "xgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n",
    "                                     max_depth=3, min_child_weight=0,\n",
    "                                     gamma=0, subsample=0.7,\n",
    "                                     colsample_bytree=0.7,\n",
    "                                     objective='reg:linear', nthread=-1,\n",
    "                                     scale_pos_weight=1, seed=27,\n",
    "                                     reg_alpha=0.00006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "cf7de3ad9c5009b8d8910b3905c4e81da7ac155c"
   },
   "outputs": [],
   "source": [
    "stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgboost, lightgbm),\n",
    "                                meta_regressor=xgboost,\n",
    "                                use_features_in_secondary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "9faff7eb24ade9be3df1157455fe7783f9f54924"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge: 16.0686 (0.0712)\n",
      " 2019-03-21 06:39:18.573581\n"
     ]
    }
   ],
   "source": [
    "score = cv_mae(ridge , X)\n",
    "print(\"Ridge: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "715afd46c7877d45ec45de439ecb4ad6f8925072"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO: 16.0692 (0.0711)\n",
      " 2019-03-21 06:40:15.861821\n"
     ]
    }
   ],
   "source": [
    "score = cv_mae(lasso , X)\n",
    "print(\"LASSO: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "f81fbdd0aaaf9e542f1abe9575a51043b1074f82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elastic net: 16.0687 (0.0713)\n",
      " 2019-03-21 06:42:20.960286\n"
     ]
    }
   ],
   "source": [
    "score = cv_mae(elasticnet)\n",
    "print(\"elastic net: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "5967bb4c02274454821d9adb52ab1d9018e25ccf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR: 16.2203 (0.0821)\n",
      " 2019-03-21 07:07:07.757559\n"
     ]
    }
   ],
   "source": [
    "score = cv_mae(svr)\n",
    "print(\"SVR: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "0fd22e14eac8d3fcf8f2deba32594dec6668ab0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbr: 14.7810 (0.0802)\n",
      " 2019-03-21 07:16:09.629010\n"
     ]
    }
   ],
   "source": [
    "score = cv_mae(gbr)\n",
    "print(\"gbr: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "9ec48ca9c549fedef7902e83f49a85a795a6521c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm: 14.8736 (0.0766)\n",
      " 2019-03-21 07:16:52.641360\n"
     ]
    }
   ],
   "source": [
    "score = cv_mae(lightgbm)\n",
    "print(\"lightgbm: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "e125eea899c9d3d586e845ca921cdae6aefe6f70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost: 14.7975 (0.0756)\n",
      " 2019-03-21 07:24:22.152527\n"
     ]
    }
   ],
   "source": [
    "score = cv_mae(xgboost)\n",
    "print(\"xgboost: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "fb6c721a1c3796ec99082867c06d842790feaa1c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START Fit\n",
      "stack_gen\n",
      "elasticnet\n",
      "Lasso\n",
      "Ridge\n",
      "Svr\n",
      "GradientBoosting\n",
      "xgboost\n",
      "lightgbm\n"
     ]
    }
   ],
   "source": [
    "print('START Fit')\n",
    "\n",
    "print('stack_gen')\n",
    "stack_gen_model = stack_gen.fit(np.array(X), np.array(y))\n",
    "\n",
    "print('elasticnet')\n",
    "elastic_model_full_data = elasticnet.fit(X, y)\n",
    "\n",
    "print('Lasso')\n",
    "lasso_model_full_data = lasso.fit(X, y)\n",
    "\n",
    "print('Ridge')\n",
    "ridge_model_full_data = ridge.fit(X, y)\n",
    "\n",
    "print('Svr')\n",
    "svr_model_full_data = svr.fit(X, y)\n",
    "\n",
    "print('GradientBoosting')\n",
    "gbr_model_full_data = gbr.fit(X, y)\n",
    "\n",
    "print('xgboost')\n",
    "xgb_model_full_data = xgboost.fit(X, y)\n",
    "\n",
    "print('lightgbm')\n",
    "lgb_model_full_data = lightgbm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "fd482391864349712396694bf271dbf42a37d785"
   },
   "outputs": [],
   "source": [
    "def blend_models_predict(X):\n",
    "    return ((0.05 * elastic_model_full_data.predict(X)) + \\\n",
    "            (0.05 * lasso_model_full_data.predict(X)) + \\\n",
    "            (0.05 * ridge_model_full_data.predict(X)) + \\\n",
    "            (0.1 * svr_model_full_data.predict(X)) + \\\n",
    "            (0.15 * gbr_model_full_data.predict(X)) + \\\n",
    "            (0.15 * xgb_model_full_data.predict(X)) + \\\n",
    "            (0.15 * lgb_model_full_data.predict(X)) + \\\n",
    "            (0.3 * stack_gen_model.predict(np.array(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "1b88b4e58530d95b4e6227e864bc17072fdd4024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE score on train data:\n",
      "13.972378983793371\n"
     ]
    }
   ],
   "source": [
    "print('MAE score on train data:')\n",
    "print(mae(y, blend_models_predict(X)))\n",
    "predictions = blend_models_predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "2ec9b0d93d4766749983550dcc0417f0bb676f3e"
   },
   "outputs": [],
   "source": [
    "test_data_sub1 = pd.DataFrame()\n",
    "test_data_sub1['id'] = test_id\n",
    "test_data_sub1['score'] =  predictions\n",
    "test_data_sub1.columns = ['id','score']\n",
    "\n",
    "test_data_sub1['score'] = test_data_sub1['score'].apply(lambda x: int(np.round(x)))\n",
    "test_data_sub1[['id','score']].to_csv('lgb_xgb_stacking.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
